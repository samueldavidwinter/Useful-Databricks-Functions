#View Hive Metastore Data
  #With SQL 
%sql
USE raw_vault;
SELECT * FROM hub_activity LIMIT 3


  #With Python
%sql
use raw_vault;

%python
df.spark.sql("SELECT * FROM hub_activity")

  # Read to pandas from DFBS  (Can use locally after DB token is configured)
  spark.sql("SELECT * FROM sandbox_dr.postcode_master").toPandas()



# Write to Local (After DB token is configured)

    dbfs cp dbfs:/FileStore/goodiesFromSam/3Geneysis/part-00000-tid-6235184272482111912-f2c53184-c1ff-479a-af80-5b17e8e65461-25-1-c000.snappy.parquet downloads/your_file.csv



#Writing and Reading Dataframes to parquet in dfbs
   # Remove the file if it exists
      dbutils.fs.rm("/tmp/databricks-df-example.parquet", True)
      unionDF.write.format("parquet").save("/tmp/databricks-df-example.parquet")


      parquetDF = spark.read.format("parquet").load("/tmp/databricks-df-example.parquet")
      parquetDF.show(truncate=False)
