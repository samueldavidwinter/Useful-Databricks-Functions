# Creating a Delta Lake table from a dataframe (to do so, we need to specify the format as delta)
   # Load a file into a dataframe
df = spark.read.load('/data/mydata.csv', format='csv', header=True)

  # Save the dataframe as a delta table
delta_table_path = "/delta/mydata"
df.write.format("delta").save(delta_table_path)


Updating a delta lake table
  # Replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode:
new_df.write.format("delta").mode("overwrite").save(delta_table_path)

  # Add rows from a dataframe to an existing table by using the append mode:
new_rows_df.write.format("delta").mode("append").save(delta_table_path)


  # Making conditional updates
from delta.tables import *
from pyspark.sql.functions import *

    # Create a deltaTable object
deltaTable = DeltaTable.forPath(spark, delta_table_path)

    # Update the table (reduce price of accessories by 10%)
deltaTable.update(
    condition = "Category == 'Accessories'",
    set = { "Price": "Price * 0.9" })


  # Querying Previous Versions of a Table

df = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)

  # Alternatively, you can specify a timestamp by using the timestampAsOf option:
df = spark.read.format("delta").option("timestampAsOf", '2022-01-01').load(delta_table_path)




# Py instead of SQL from DatabaseTables

use dr_report;



%python
df = spark.sql("SELECT * FROM case_base")

# Create new sql table


df.createOrReplaceTempView('case_base2')

df2 = spark.sql('SELECT * FROM case_base2 WHERE flightduration < 100')


# Create a list of databrick tables 
db_list = [x[0] for x in spark.sql("SHOW DATABASES").rdd.collect()]
 
 
for i in db_list:
  spark.sql("SHOW TABLES IN {}".format(i)).createOrReplaceTempView(str(i)+"TablesList")
  
# Create a dataframe with databrick tables 
df = spark.sql("show tables in {}".format("raw_vault"))

 # Create a column with databrick tables matching a patern
hub_dataframe_allTables = df.filter(df.tableName.contains("_1"))
hub_dataframe_allTablesOnly = df.select("tableName")

#Combining dataframes together
unionDF = df1.union(df2)
unionDF.show(truncate=False)

#Writing and Reading Dataframes to parquet in dfbs
   # Remove the file if it exists
dbutils.fs.rm("/tmp/databricks-df-example.parquet", True)
unionDF.write.format("parquet").save("/tmp/databricks-df-example.parquet")


parquetDF = spark.read.format("parquet").load("/tmp/databricks-df-example.parquet")
parquetDF.show(truncate=False)


