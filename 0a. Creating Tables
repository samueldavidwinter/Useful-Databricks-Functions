#Create Tables
    #Global Unmanaged/external table 
    df.repartition(1).write.format("parquet").option("header", "true").save("dbfs:/FileStore/goodiesFromSam")
  - Spark will manage the metadata, we control the data location
  - Available across all clusters 
  - This is set via the path option 
  - repartition, optional, writes into one location/frame

    #Global Managed table
    df.write.saveAsTable("my_table")
  - Spark manages both the data and the metadata. 
  - A global managed table is available across all clusters.

    #Local Table / Temp View
    df.createOrReplaceTempView()
    - Not accessible from other clusters or workbooks 
    - Not registered in the meta-store
    df2 = spark.sql('SELECT * FROM case_base2 WHERE flightduration < 100')
   
    
    # Global View
    df.createOrReplaceGlobalTempView("my_global_view")
    - Accessible from other clusters or workbooks ( spark.read.table("global_temp.my_global_view") )
    - Not registered in the meta-store
    
    # Global Permanent View 
    spark.sql("CREATE VIEW permanent_view AS SELECT * FROM table")
    - The view definition is recorded in the underlying metastore. 
    - One can only create permanent view on global managed table or global unmanaged table. 
    - Permanent views are only available in SQL API â€” not available in dataframe API





