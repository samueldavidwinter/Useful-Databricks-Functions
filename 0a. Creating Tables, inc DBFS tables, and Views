#Create Tables
    #Global Unmanaged/external table 
    df.repartition(1).write.format("parquet").option("header", "true").save("dbfs:/FileStore/goodiesFromSam")
  - Spark will manage the metadata, we control the data location
  - Available across all clusters 
  - Turing on as an external table is set via the path/save option 
  - repartition, optional, writes into one location/frame

    #Global Managed table
    df.write.saveAsTable("my_table")
  - Spark manages both the data and the metadata. 
  - A global managed table is available across all clusters.

    #Local Table / Temp View
    df.createOrReplaceTempView()
    - Not accessible from other clusters or workbooks 
    - Not registered in the meta-store
    df2 = spark.sql('SELECT * FROM case_base2 WHERE flightduration < 100')
   
    
    # Global View
    df.createOrReplaceGlobalTempView("my_global_view")
    - Accessible from other clusters or workbooks ( spark.read.table("global_temp.my_global_view") )
    - Not registered in the meta-store
    
    # Global Permanent View 
    spark.sql("CREATE VIEW permanent_view AS SELECT * FROM table")
    - The view definition is recorded in the underlying metastore. 
    - One can only create permanent view on global managed table or global unmanaged table. 
    - Permanent views are only available in SQL API â€” not available in dataframe API
    
    # Creating from DBFS
    import pandas as pd
    from io import StringIO

        data = """
        ***copy and paste the output from databricks here, it'll look a bit like: ***

        CODE,L,PS
        5d8A,N,P60490
        5d8b,H,P80377
        5d8C,O,P60491
        """

        df = pd.read_csv(StringIO(data), sep=',')
        #print(df)
        df.to_csv('/dbfs/FileStore/***your path**.txt')

        pandas_df = pd.read_csv("/dbfs/FileStore/NJ/file1.txt", header='infer')





